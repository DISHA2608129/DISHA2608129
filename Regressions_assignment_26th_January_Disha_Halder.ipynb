{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcvLLtQX/zjEzXdnsEFPzz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DISHA2608129/DISHA2608129/blob/main/Regressions_assignment_26th_January_Disha_Halder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regressions?"
      ],
      "metadata": {
        "id": "EMHDYSVjofKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Simple Linear Regression is a statistical technique which has to study the relationship between two variables,such as an independent variable (predictor) and another is dependent variable (outcome). It assumes the relationship between them is linear, represented by the equation.\n",
        "y=mx+c\n",
        "Here,\n",
        "ùëö is the slope (rate of change)\n",
        "c is the intercept (value of y when x=0).\n",
        "Example:\n",
        "Suppose a business has wanted to predict monthly sales (y) based on advertising expenditure (x). By analyzing past data, simple linear regression can find the equation that predicts sales given a specific advertising budget. For instance, if\n",
        "y=50x+2000, spending ¬£1000 on advertising predicts ¬£52,000 in sales.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1kZFlbkkoh1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regressions?"
      ],
      "metadata": {
        "id": "uzBLzhADpE4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regressions have consisit of various key assumptions, which has been listed below.   \n",
        "Linearity: The relationship between the independent (\\(x\\)) and dependent (\\(y\\)) variables is linear.  \n",
        "Independencey: The observations are independent from each other (no autocorrelation).  \n",
        "Homoscedasticity: The variance of the residuals (errors) is constant across all independent variable values.  \n",
        "Normality of Residuals: The residuals (differences between observed and predicted \\(y\\)) are normally distributed.  \n",
        "No Multicollinearity: As here simple regressions have been considered, there's only one independent variable, so multicollinearity isn't relevant here, which ensured predictions and validity of the model."
      ],
      "metadata": {
        "id": "owc12uLzpZ4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent the equation Y = mX + c?"
      ],
      "metadata": {
        "id": "FsU6kS_NqEMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the equation\n",
        "Y=mX+c, the coefficient m represents the slope of the line. It indicates the rate of change in the dependent variable (Y) for a one-unit increase in the independent variable (X).\n",
        "If\n",
        "ùëö > 0\n",
        "the value of Y increases as X increases (based on positive relationship).\n",
        "If\n",
        "ùëö < 0\n",
        "Y decreases as X increases (negative relationship).\n",
        "If m = 0\n",
        "Y does not change with X (no relationship).\n",
        "For example, in predicting test scores (Y) based on study hours (X),\n",
        "ùëö= 10, means each additional hour of studying increases the test score by 10 points."
      ],
      "metadata": {
        "id": "wNaff7gCqETB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent the equation Y = mX + c?"
      ],
      "metadata": {
        "id": "z5YwiZ7qpO_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the equation\n",
        "Y=mX+c\n",
        "the intercept c represents the value of Y when the independent variable (X) is equal to 0. It is the point where the regression line crosses the Y-axis.\n",
        "Significance\n",
        "It provides a baseline value of Y when no effect from X is present.\n",
        "In practical terms, c shows the starting value of Y before X influences it.\n",
        "Example:\n",
        "If\n",
        "Y=5X+20,\n",
        "then,\n",
        "c=20. This means when\n",
        "X=0,\n",
        "Y=20.\n",
        "For instance, if\n",
        "X represents study hours and Y represents test scores, a score of 20 is expected without studying."
      ],
      "metadata": {
        "id": "VMVZuOHZrqbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in a Simple Linear Regression?"
      ],
      "metadata": {
        "id": "oqVgHtKWszpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope (m) in a Simple Linear Regression equation\n",
        "(Y=mX+c) can be calculated using the formula:\n",
        "\n",
        "    m = ‚àë(Xi-X)(Yi-Y)-‚àë(Xi-X)^2\n",
        "Where:\n",
        "Xi and Yi are the individual data points for the independent and dependent variables.\n",
        "X and Y are the means of X and Y, respectively."
      ],
      "metadata": {
        "id": "rzX0A9EbtOio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least square methods in Simple Linear Regressions?"
      ],
      "metadata": {
        "id": "2FI7HIFtxxdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the Least Squares Method in Simple Linear Regression is to find the best-fitting line (Y=mX+c) that minimizes the error between the predicted values (^Y) nd the observed values (Y) in the dataset.\n",
        "Key Objective:\n",
        "It minimizes the sum of squared residuals, where a residual is the difference between the actual value (Yi) and the predicted value (Yi^)\n",
        "Residual = (Yi) - (Yi^)\n",
        "The least square methid has maintained:\n",
        "Minimisation value = ‚àë(Yi) - (Yi^)^2"
      ],
      "metadata": {
        "id": "0dpLh3CNyDQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?"
      ],
      "metadata": {
        "id": "5cbYf2Kv0Icq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficient of determination (R¬≤) is a statistical measure that explains how well the independent variable (X) predicts the dependent variable (Y) in a Simple Linear Regression. It represents the proportion of the variance in\n",
        "Y that is explained by X.\n",
        "\n",
        "              R¬≤ = 1 - (SSres- SStot)\n",
        "\n",
        "        SSres = The sum of squared residuals (unexplained variance).\n",
        "        SStot = Total sum of squares (total variance in Y)\n",
        "\n",
        "R¬≤ =1: Perfect fit, then\n",
        "X explains all the variation in Y.\n",
        "R¬≤= 0: No fit;\n",
        "X explains none of the variation in Y.\n",
        "R¬≤= 0.7, this means that,\n",
        "70% of the variation in Y is explained by X, while 30% is due to other factors or noise."
      ],
      "metadata": {
        "id": "OgP24kmx0cV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regressions?"
      ],
      "metadata": {
        "id": "w9t5_Y6D1haD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regression is a statistical technique used to model the relationship between one dependent variable (Y) and two or more independent variables (X1, X2, X3....Xn). It extends Simple Linear Regression by allowing multiple predictors to explain the variation in the dependent variable. The general form of the equation is-\n",
        "       Y = b0 + b1X1 + b2X2 + b3X3 +.....+ bnXn\n",
        "\n",
        "       where,\n",
        "       Y = Dependent variable.\n",
        "       X1, X2,‚Ä¶,Xn = Independent variables.\n",
        "       b0 = Intercept\n"
      ],
      "metadata": {
        "id": "mbt84RCk15qU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "91iJvLm4251f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Use Case\n",
        "Simple Linear Regression: Best for analyzing the impact of a single variable.\n",
        "Multiple Linear Regression: Suitable for understanding relationships with multiple predictors.\n",
        "2. Complexity:\n",
        "Simple Linear Regression: Simpler and easier to interpret due to a single predictor.\n",
        "Multiple Linear Regression: More complex but can capture the combined influence of multiple factors on Y.\n",
        "3. Number of Independent Variables\n",
        "Simple Linear Regression: Involves only one independent variable (X) to predict the dependent variable (Y).\n",
        "Example: Y = mX + c.\n",
        "Multiple Linear Regression: Involves two or more independent variables (X1,X2, X3,...Xn) to predict the dependent variable (Y)."
      ],
      "metadata": {
        "id": "XINbJqIz3G3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "YDSz5gug3600"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independence: The observations (data points) are independent of each other. There should be no autocorrelation of residuals.\n",
        "No Multicollinearity: The independent variables should not be highly correlated with each other. High correlation between predictors can make it difficult to determine the individual effect of each variable on Y.\n",
        "Homoscedasticity: The variance of the residuals (errors) should be constant across all levels of the independent variables. If residuals fan out or contract at different levels of predictors, it indicates heteroscedasticity.\n",
        "Normality of Residuals: The residuals (errors between the observed and predicted values) should be approximately normally distributed. This ensures reliable statistical inference for significance testing.\n",
        "No Outliers or Influential Data Points: Outliers or highly influential points should not disproportionately affect the regression model. They can skew the results and distort the relationship between the variables.\n",
        "Linearity: The relationship between the dependent variable (Y) and each independent variable (X1, X2, X3,.....,Xn) is linear. This means the change in\n",
        "Y is proportional to the change in the independent variables."
      ],
      "metadata": {
        "id": "lMvt3azj4Hft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?"
      ],
      "metadata": {
        "id": "2lOoHXJl4d0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity is denoted as the situation where the variance of the residuals or errors in a regression model is not constant across all levels of the independent variables. As the values of the predictors increase or decrease, the spread or variability of the residuals changes. it has violates the assumption of homoscedasticity, which needed that the residuals have constant variance.\n",
        "The ways Heteroscedasticity affects Multiple Linear Regression\n",
        "Bias in Standard Errors\n",
        "Heteroscedasticity leads to inaccurate estimates of the standard errors of the regression coefficients. This affects the reliability of hypothesis tests (such as t-tests) and confidence intervals. The results may suggest statistically significant predictors when they are not, or vice versa.\n",
        "Incorrect Inferences\n",
        "Heteroscedasticity can lead to incorrect conclusions about the importance of independent variables. In some cases, it may overstate the significance of predictors or underestimate the true uncertainty in the regression model.\n",
        "Impact on Model Fit\n",
        "Heteroscedasticity can distort the model‚Äôs fit to the data, leading to misleading conclusions about how well the model represents the data.\n",
        "Addressing Heteroscedasticity:\n",
        "Transformation\n",
        "Transforming the dependent variable (e.g., using a logarithmic or square root transformation) can stabilize the variance of residuals.\n",
        "Weighted Least Squares\n",
        "This method assigns weights to observations to account for varying residual variance."
      ],
      "metadata": {
        "id": "5371UUZc4qQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?"
      ],
      "metadata": {
        "id": "XzPMO6wh5xlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For improving a Multiple Linear Regression model with high multicollinearity, there are several strategies can be used  to reduce the impact of multicollinearity and stabilize the model.\n",
        "Remove Highly Correlated Variables\n",
        "If two or more independent variables are highly correlated, it is needed to consider removing one of them. One can assess correlation by calculating the correlation matrix or using a Variance Inflation Factor (VIF) to identify which variables are causing multicollinearity.Removing redundant predictors helps to simplify the model and reduce collinearity.\n",
        "Combine Correlated Variables\n",
        "Instead of removing correlated variables,combine all of the variables into a single composite variable. For example, if you have two highly correlated variables (like \"height\" and \"weight\"), combine them into a new variable, such as Body Mass Index (BMI), that captures the relationship between both variables.\n",
        "Principal Component Analysis (PCA)\n",
        "PCA is a technique that transforms correlated variables into a smaller number of uncorrelated variables called principal components. These components can then be used in regression model, reducing multicollinearity and making the model more stable.\n",
        "Use Regularization Techniques (Lasso Regression)\n",
        "Ridge Regression (L2 regularization) adds a penalty term to the regression model which shrinks the coefficients of highly correlated predictors, reducing  impact, which stabilize the model.\n",
        "Lasso Regression (L1 regularization) also adds a penalty term but can shrink some coefficients to zero, effectively removing less important variables from the model. This can help both reduce multicollinearity and perform feature selection."
      ],
      "metadata": {
        "id": "8bWMEst852xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?"
      ],
      "metadata": {
        "id": "u04dIMeq7MlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding\n",
        "One-hot encoding creates binary (0 or 1) columns for each category in a categorical variable. Each column represents whether a particular category is present for a given observation.\n",
        "When to use: It‚Äôs best for nominal (unordered) categorical variables, where there is no inherent order or hierarchy between the categories.\n",
        "Example,\n",
        "For a variable Color with categories Red, Green, and Blue:\n",
        "Red becomes\n",
        "[1,0,0]\n",
        "Green becomes\n",
        "[0,1,0]\n",
        "Blue becomes\n",
        "[0,0,1]\n",
        "Pitfall: This can lead to a large number of columns if the categorical variable has many levels (high cardinality).\n",
        "Label Encoding\n",
        "What it is: Label encoding assigns a unique integer to each category in a categorical variable. This method is suitable for ordinal variables, where the categories have a meaningful order.\n",
        "Best time to use: It‚Äôs best for ordinal (ordered) categorical variables where the categories have a rank or order (e.g., low, medium, high).\n",
        "Example,\n",
        "For a variable Size with categories Small, Medium, and Large:\n",
        "Small becomes 0\n",
        "Medium becomes 1\n",
        "Large becomes 2\n",
        "Pitfall: Label encoding can introduce an artificial ordering to nominal variables (i.e., unordered categories), which may not be meaningful.\n",
        "Ordinal Encoding\n",
        "What it is: Similar to label encoding, but specifically used for ordinal variables where the categories have a natural order (e.g., ratings like Poor, Average, Good).\n",
        "When to use: It‚Äôs used when you want to preserve the order of the categories in an ordinal variable.\n",
        "Example,\n",
        "For a variable Rating with categories Poor, Average, Good:\n",
        "Poor becomes 0\n",
        "Average becomes 1\n",
        "Good becomes 2"
      ],
      "metadata": {
        "id": "ibevMzCd8Bc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "xg177VUR8lp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of Multiple Linear Regression, interaction terms are used to model the combined effect of two or more independent variables on the dependent variable that is different from their individual (additive) effects. Interaction terms are especially useful when the effect of one independent variable on the dependent variable depends on the value of another independent variable.\n",
        "Role of Interaction Terms\n",
        "Capturing Synergistic Effects\n",
        "Interaction terms allow you to capture scenarios where the relationship between an independent variable and the dependent variable is not constant, but rather changes depending on the level of another independent variable.\n",
        "For example, the effect of advertising budget on sales may depend on the region where the product is being sold. The effect of advertising might be stronger in one region than in another, so an interaction term can model this dependency.\n",
        "Enhancing Model Accuracy\n",
        "Including interaction terms in the regression model can improve its fit by accounting for complex relationships between variables that might otherwise be missed. If these interactions are significant, the model becomes more accurate in predicting the outcome.\n",
        "Improving Interpretability\n",
        "Interaction terms allow for more nuanced interpretations of how predictors work together. Without interaction terms, the effect of each predictor is assumed to be independent of the others, which may oversimplify real-world relationships."
      ],
      "metadata": {
        "id": "mOSyAf_U8w5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "KK8y0m0P9EvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Simple Linear Regression, the model has one independent variable X and the dependent variable Y. The regression equation is:\n",
        "                       Y=mX+c\n",
        "Y is the dependent variable (target).\n",
        "X is the independent variable (predictor).\n",
        "m is the slope (the effect of X on Y).\n",
        "c is the intercept (the value of Y when X=0).\n",
        "Interpretation of the intercept c in Simple Linear Regression\n",
        "The intercept represents the predicted value of Y when X=0.\n",
        "If,\n",
        "X=0 is within the meaningful range of your data, the intercept gives a clear understanding of what\n",
        "Y would be in this situation. For example, if predicting house prices based on size of the house, the intercept would represent the price of a house with a size of 0 (though it might not be practically meaningful).\n",
        "\n",
        "Intercept in Multiple Linear Regression:\n",
        "In Multiple Linear Regression, the model has more than one independent variable. The equation looks like:\n",
        "Y = b0 + b1X1 +b2X2 + b3X3 +...+ bnXn\n",
        "Where:\n",
        "Y is the dependent variable (target).\n",
        "X1, X2...Xn are the independent variables (predictors).\n",
        "bo is the intercept (the value of Y when all Xi = 0)\n",
        "b1, b2, b3...bn are the coefficients (representing the effect of each independent variable on Y)."
      ],
      "metadata": {
        "id": "gE_3xBke9Z57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?"
      ],
      "metadata": {
        "id": "oA6WczOHE3WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slope in regression analysis, often denoted as m or b1 (for simple and multiple linear regression, respectively), plays a crucial role in determining the relationship between the independent variable(s) and the dependent variable. Its significance and effect on predictions are essential for understanding how changes in the predictors influence the target variable.\n",
        "Significance of the Slope in Regression Analysis:\n",
        "In regression models, the slope represents the rate of change in the dependent variable (Y) for a one-unit change in the independent variable(s) (X). It indicates the strength and direction of the relationship between the predictor(s) and the outcome.\n",
        "Simple Linear Regression:\n",
        "The general equation for simple linear regression is\n",
        "Y=mX+c\n",
        "m is the slope.\n",
        "X is the independent variable.\n",
        "c is the intercept.\n",
        "In this case, the slope tells how much\n",
        "Y will change for each one-unit increase in X. A positive slope means that as\n",
        "X increases, Y also increases, and a negative slope means that as\n",
        "X increases, Y decreases.\n",
        "Multiple Linear Regression:\n",
        "The general equation for multiple linear regression is:\n",
        "   Y = b0 + b1X1 + b2X2 + b3X3 +....+ bnXn\n",
        "b1, b2...bn are the slopes of the variables X1, X2....Xn."
      ],
      "metadata": {
        "id": "QddLRlxlFErQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?"
      ],
      "metadata": {
        "id": "WaVKe8SOGvpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contextual Meaning and Practical Significance\n",
        "The intercept's practical significance depends on the context and whether the values of the independent variables being zero are meaningful.\n",
        "In some cases, an intercept of zero or close to zero may make sense. For example, in a model predicting sales based on advertising spend, an intercept close to zero might indicate that when no money is spent on advertising, the sales are almost zero.\n",
        "In other cases, such as predicting house price, an intercept may not have a meaningful interpretation because the scenario where all independent variables are zero (e.g., no square footage or no age) is unrealistic. However, the intercept still serves as a reference point for the relationship between the predictors and the dependent variable.\n",
        "Understanding Relationships Between Variables\n",
        "The intercept, when considered alongside the slopes (coefficients), gives context to how the independent variables influence the dependent variable. For instance:\n",
        "If the intercept is large, it might indicate a high baseline value for Y (even without any input from the predictors).\n",
        "The slope values show the magnitude and direction of the relationship between each predictor and Y. Together with the intercept, the slopes complete the story of how the independent variables influence the dependent variable.\n",
        "Model Calibration\n",
        "The intercept helps calibrate the model by adjusting the predicted value of Y to match the observed data. It ensures that the model aligns with the actual starting point of the dependent variable and provides an anchor from which the effects of the predictors are measured."
      ],
      "metadata": {
        "id": "qrVhKxsQHBqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R¬≤ as a sole measure of model performance?"
      ],
      "metadata": {
        "id": "QaE41bPzHWBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does Not Indicate Causality:\n",
        "R¬≤ shows the strength of the relationship between the independent variables and the dependent variable, but it does not prove a causal relationship. A high R¬≤ might indicate that the model fits the data well, but it does not tell you if one variable is causing changes in another.\n",
        "Sensitive to Outliers:\n",
        "R¬≤ is highly sensitive to outliers in the data. Even a  extreme values can dramatically increase or decrease R¬≤, leading to misleading conclusions about model performance. Outliers can be distort the regression line and affect the calculated goodness of fit.\n",
        "Can Be Misleading in Non-Linear Relationships:\n",
        "R¬≤ is most useful in linear regression models. If the underlying relationship between the variables is non-linear, R¬≤ can still be high, but the model might not be a good representation of the actual relationship. It will fail to capture complex patterns, leading to poor predictions.\n",
        "Does Not Determined Model Complexity:\n",
        "A high R¬≤ value does not necessarily indicate a good model, especially when the model is overfitting. Overfitting occurs when a model fits the training data too closely, capturing noise and random fluctuations that do not generalize well to new data. In such cases, the model may have a high R¬≤ but perform poorly on unseen data."
      ],
      "metadata": {
        "id": "pk_Kia_QHhke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?"
      ],
      "metadata": {
        "id": "-oEzAP-dH5cO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible Causes of Large Standard Errors:\n",
        "A large standard error could be a sign of several issues in the regression model.\n",
        "Multicollinearity: If independent variables are highly correlated with each other, it can cause multicollinearity, leading to unstable estimates of the regression coefficients and thus larger standard errors. When predictors are highly correlated, it becomes difficult to isolate the individual effect of each predictor on the dependent variable.\n",
        "Small sample size: In small datasets, there is generally more sampling variability, which can lead to large standard errors for the coefficients. Larger sample sizes typically result in more precise estimates.\n",
        "Outliers or influential data points: Outliers or influential observations can skew the estimates of the regression coefficients, leading to large standard errors. These extreme values can disproportionately affect the results of the regression.\n",
        "Interpretation of Standard Error:\n",
        "The standard error of a regression coefficient represents the variability or precision of that estimate. It quantifies how much the estimated coefficient (Œ≤) is expected to vary from the true population value due to sampling error.\n",
        "Large standard error: Implies that the estimated coefficient is less precise and there is more uncertainty about the true value of the coefficient.\n",
        "Small standard error: Indicates that the estimated coefficient is more precise, and we can be more confident that the estimate is close to the true population value."
      ],
      "metadata": {
        "id": "jDDIUD8vIGwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?"
      ],
      "metadata": {
        "id": "or9xRvTvIZ_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity refers to the situation where the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variable(s). When heteroscedasticity is present, it violates one of the key assumptions of linear regression, which assumes homoscedasticity (constant variance of residuals). Identifying and addressing heteroscedasticity is crucial because it can lead to inefficient estimates of the regression coefficients, unreliable statistical tests, and incorrect inferences.\n",
        "\n",
        "How to Identify Heteroscedasticity in Residual Plots:\n",
        "Residual plots are a valuable tool for diagnosing heteroscedasticity. To create a residual plot, plot the residuals (the differences between observed and predicted values) on the y-axis and the corresponding fitted values (predicted values) or the independent variable(s) on the x-axis.\n",
        "\n",
        "Key Features of Residual Plots:\n",
        "Random Pattern (No Heteroscedasticity):\n",
        "\n",
        "In the absence of heteroscedasticity, the residuals should be scattered randomly around the horizontal line at zero.\n",
        "The spread of the residuals should remain relatively constant across all values of the predicted values or independent variables.\n",
        "A homoscedastic plot typically shows no visible pattern and no funneling or cone shape.\n",
        "Funnel or Cone Shape (Indication of Heteroscedasticity):\n",
        "\n",
        "If the spread of residuals increases or decreases as the fitted values or independent variables change, this suggests heteroscedasticity.\n",
        "For example, if residuals become more spread out as the fitted values increase (forming a \"funnel\" shape), it indicates that the variance of errors increases as the value of the independent variable or the predicted outcome increases.\n",
        "Conversely, if the residuals become more tightly packed as the fitted values increase, it would indicate decreasing variance at higher values.\n",
        "Patterns:\n",
        "\n",
        "Sometimes, heteroscedasticity can appear as a systematic pattern in the residual plot, such as curves or other systematic trends, indicating that the model might not be capturing some aspect of the data.\n",
        "Example of a Residual Plot Showing Heteroscedasticity:\n",
        "Imagine plotting residuals against fitted values, and the residuals start to form a \"V\" or \"funnel\" shape, where the spread of residuals increases as the fitted values get larger. This is a clear sign of heteroscedasticity.\n",
        "Alternatively, if you plot residuals against an independent variable and notice that the residuals become more spread out or more concentrated as the independent variable increases, this also suggests heteroscedasticity.\n",
        "Why Is It Important to Address Heteroscedasticity?\n",
        "Unreliable Parameter Estimates:\n",
        "\n",
        "In the presence of heteroscedasticity, the ordinary least squares (OLS) estimates of the regression coefficients remain unbiased, but they are no longer efficient. This means that the model‚Äôs estimates may not be as precise as they could be, leading to larger standard errors and less reliable coefficients.\n",
        "As a result, confidence intervals may be too wide, and hypothesis tests (e.g., t-tests for coefficients) may have incorrect significance levels, leading to potentially incorrect inferences.\n",
        "Inaccurate Standard Errors and Hypothesis Testing:\n",
        "The standard errors of the regression coefficients are affected by heteroscedasticity, which means that hypothesis tests (e.g., t-tests) and p-values based on these errors may be misleading.\n",
        "Wald tests and confidence intervals can give incorrect results when heteroscedasticity is present, increasing the risk of Type I and Type II errors.\n",
        "Efficiency of the Model:\n",
        "One of the key advantages of linear regression is that it produces efficient estimates under the assumption of homoscedasticity. When heteroscedasticity is present, OLS estimates are no longer efficient, meaning that there may be more precise ways to estimate the coefficients using other methods, like generalized least squares (GLS).\n",
        "How to Address Heteroscedasticity:\n",
        "If heteroscedasticity is detected, there are several strategies that can be used to address it:\n",
        "Transform the Dependent Variable:\n",
        "Apply a logarithmic transformation (e.g.,log(Y)) or other transformations (such as square root or inverse) to the dependent variable to stabilize the variance of residuals.\n",
        "Use Robust Standard Errors:\n",
        "Instead of using standard OLS standard errors, you can use robust standard errors (also known as heteroscedasticity-consistent standard errors), which adjust the standard errors to account for heteroscedasticity. This method can help obtain more reliable p-values and confidence intervals when heteroscedasticity is present.\n",
        "Weighted Least Squares (WLS):\n",
        "If the form of heteroscedasticity is known or can be modeled, you can use weighted least squares regression, where each observation is given a weight based on its variance. This can correct for the varying variance of residuals.\n",
        "Generalized Least Squares (GLS):\n",
        "For more complex heteroscedasticity, generalized least squares is another approach that can be used to address heteroscedasticity by modeling the error variance structure explicitly.\n",
        "Reconsider Model Specification:\n",
        "Sometimes, heteroscedasticity is a sign that the model is misspecified, such as missing important predictors or not accounting for non-linear relationships. You may need to reconsider the choice of variables or the functional form of the model."
      ],
      "metadata": {
        "id": "Jq2LTNRXI4XH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?"
      ],
      "metadata": {
        "id": "2g0ZEmNXKLn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding R¬≤ and Adjusted R¬≤:\n",
        "R¬≤ (Coefficient of Determination)\n",
        "Measures the proportion of the variance in the dependent variable that is explained by the independent variables. A high R¬≤ suggests that the model explains a large portion of the variance in the outcome variable.\n",
        "Adjusted R¬≤\n",
        "Adjusts R¬≤ to account for the number of predictors (independent variables) in the model. Unlike R¬≤, which always increases as more variables are added, adjusted R¬≤ penalizes for adding irrelevant or unnecessary variables, and it can decrease if the added variables do not improve the model's fit substantially.\n",
        "Meaning of to Have a High R¬≤ but Low Adjusted R¬≤\n",
        "High R¬≤\n",
        "This suggests that the model fits the data well and explains a large portion of the variance in the dependent variable.\n",
        "Low Adjusted R¬≤\n",
        "This indicates that the additional predictors in the model do not provide much explanatory power, and the model is likely being overfit with too many variables. The increase in R¬≤ due to additional variables is not large enough to offset the penalty applied by the adjusted R¬≤.\n",
        "In essence, the model is capturing noise in the data rather than true underlying patterns, which can happen when:\n",
        "There are too many predictors in the model, many of which are irrelevant or weakly related to the dependent variable.\n",
        "The model is becoming more complex without adding real value to the prediction, leading to overfitting.\n",
        "Why Can This Happen?\n",
        "Overfitting:\n",
        "While adding more predictors to the model, R¬≤ can only stay the same or increase, even if the new variables are not meaningful. This can lead to a high R¬≤ value, but the adjusted R¬≤ will penalize the inclusion of unnecessary predictors and will likely decrease or stay low if those predictors don't improve the model's true explanatory power.\n",
        "Irrelevant Predictors:\n",
        "Including irrelevant or redundant variables in the model can inflate R¬≤, but those variables do not contribute significantly to explaining the dependent variable. The adjusted R¬≤ corrects for this by factoring in the number of predictors.\n",
        "Interpretation and Implications:\n",
        "Overfitting Indicator:\n",
        "A large gap between R¬≤ and adjusted R¬≤ suggests that the model may be overfitting the data, meaning it‚Äôs capturing noise rather than meaningful patterns. While the model might seem to fit well in the training data (reflected in the high R¬≤), it could perform poorly on new, unseen data.\n",
        "Model Complexity:\n",
        "The low adjusted R¬≤ suggests that the model is too complex relative to the amount of useful information it‚Äôs extracting. This could be due to too many predictors or a lack of generalizability.\n",
        "What to Do About It:\n",
        "To address this situation:\n",
        "Remove Irrelevant Variables: Consider removing predictors that do not contribute significantly to the model's performance, which can be done using techniques like stepwise regression or LASSO (Least Absolute Shrinkage and Selection Operator).\n",
        "Simplify the Model: Reevaluate the number of predictors and consider simplifying the model to include only the most meaningful variables. Use domain knowledge or statistical tests to guide variable selection.\n",
        "Cross-Validation: Use techniques like cross-validation to assess how well the model generalizes to unseen data. Overfitting often becomes apparent when the model's performance on test data is much worse than its performance on the training data.\n",
        "Check for Multicollinearity: High multicollinearity between predictors can artificially inflate R¬≤. Check for multicollinearity using Variance Inflation Factor (VIF) and remove or combine highly correlated variables."
      ],
      "metadata": {
        "id": "hXFvNE4gKf0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Why is it important to scale variables in Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "F3IxM2MKMK6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equal Weight for Each Predictor:\n",
        "When the variables are on different scales (e.g., one predictor is in thousands, another is between 0 and 1), the model may give more importance to predictors with larger magnitudes, even if they are not more important.\n",
        "Scaling (such as standardizing or normalizing) brings all predictors to a common scale, ensuring that each predictor contributes equally to the model, regardless of its original units or range.\n",
        "Improves Model Convergence and Stability:\n",
        "Gradient Descent (used for fitting models) and other iterative algorithms are sensitive to the scale of the input features. When the scales of predictors vary greatly, the model‚Äôs optimization process might converge more slowly or even fail to converge.\n",
        "Scaling helps the model converge faster and improves the numerical stability of the optimization process, leading to more reliable results.\n",
        "Interpretation of Coefficients:\n",
        "In regression models, the coefficients represent the change in the dependent variable for a one-unit change in the predictor variable. When predictors have different scales, comparing the coefficients directly can be misleading.\n",
        "Standardization (scaling the variables to have zero mean and unit variance) allows for direct comparison of the magnitude of the coefficients, since they all represent the effect of a one standard deviation change in the predictor.\n",
        "Without scaling, the interpretation of the coefficients is complicated, as the scale of each variable influences how much it contributes to the dependent variable.\n",
        "Multicollinearity:\n",
        "While the variables have different scales, the correlation between predictors can be influenced by the scale of the variables. This can lead to issues with multicollinearity (high correlation among predictors), which can make it difficult to estimate the regression coefficients accurately.\n",
        "Scaling the variables can help mitigate some effects of multicollinearity, as the correlation between variables becomes more related to their actual relationships rather than their scales."
      ],
      "metadata": {
        "id": "ej3MmsQVMQmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?"
      ],
      "metadata": {
        "id": "65Xzdi7_Mi2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a type of regression analysis in which the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial. In other words, it is a form of regression that allows for the modeling of non-linear relationships by including higher-degree terms of the predictor(s) (independent variable).\n",
        "\n",
        "Formula for Polynomial Regression:\n",
        "For a single predictor\n",
        "ùëã\n",
        "X, the polynomial regression model can be written as:\n",
        "\n",
        "ùëå\n",
        "=\n",
        "ùõΩ\n",
        "0\n",
        "+\n",
        "ùõΩ\n",
        "1\n",
        "ùëã\n",
        "+\n",
        "ùõΩ\n",
        "2\n",
        "ùëã\n",
        "2\n",
        "+\n",
        "ùõΩ\n",
        "3\n",
        "ùëã\n",
        "3\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùõΩ\n",
        "ùëõ\n",
        "ùëã\n",
        "ùëõ\n",
        "+\n",
        "ùúñ\n",
        "Y=Œ≤\n",
        "0\n",
        "‚Äã\n",
        " +Œ≤\n",
        "1\n",
        "‚Äã\n",
        " X+Œ≤\n",
        "2\n",
        "‚Äã\n",
        " X\n",
        "2\n",
        " +Œ≤\n",
        "3\n",
        "‚Äã\n",
        " X\n",
        "3\n",
        " +‚ãØ+Œ≤\n",
        "n\n",
        "‚Äã\n",
        " X\n",
        "n\n",
        " +œµ\n",
        "Where:\n",
        "\n",
        "ùëå\n",
        "Y is the dependent variable.\n",
        "ùëã\n",
        "X is the independent variable.\n",
        "ùõΩ\n",
        "0\n",
        ",\n",
        "ùõΩ\n",
        "1\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùõΩ\n",
        "ùëõ\n",
        "Œ≤\n",
        "0\n",
        "‚Äã\n",
        " ,Œ≤\n",
        "1\n",
        "‚Äã\n",
        " ,‚Ä¶,Œ≤\n",
        "n\n",
        "‚Äã\n",
        "  are the regression coefficients.\n",
        "ùëõ\n",
        "n is the degree of the polynomial.\n",
        "ùúñ\n",
        "œµ is the error term (residual).\n",
        "For some multiple predictors, the model would involved polynomial terms for each predictor and their interactions, similar to the approach in Multiple Linear Regression but with higher-degree terms."
      ],
      "metadata": {
        "id": "aN45v35hMtT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?"
      ],
      "metadata": {
        "id": "zmZzlQjHNB5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial Regression and Linear Regression are both types of regression models used to predict a dependent variable, but they differ in how they model the relationship between the independent and dependent variables. Here's a breakdown of the key differences:\n",
        "\n",
        "1. Model Formulation:\n",
        "Linear Regression:\n",
        "The relationship between the independent variable(s) and the dependent variable is modeled as a straight line.\n",
        "The equation for a simple linear regression with one predictor\n",
        "ùëã\n",
        "X is:\n",
        "ùëå\n",
        "=\n",
        "ùõΩ\n",
        "0\n",
        "+\n",
        "ùõΩ\n",
        "1\n",
        "ùëã\n",
        "+\n",
        "ùúñ\n",
        "Y=Œ≤\n",
        "0\n",
        "‚Äã\n",
        " +Œ≤\n",
        "1\n",
        "‚Äã\n",
        " X+œµ\n",
        "Here,\n",
        "ùëå\n",
        "Y is the dependent variable,\n",
        "ùëã\n",
        "X is the independent variable, and\n",
        "ùõΩ\n",
        "0\n",
        "Œ≤\n",
        "0\n",
        "‚Äã\n",
        " ,\n",
        "ùõΩ\n",
        "1\n",
        "Œ≤\n",
        "1\n",
        "‚Äã\n",
        "  are the coefficients. The model assumes a linear relationship between\n",
        "ùëã\n",
        "X and\n",
        "ùëå\n",
        "Y.\n",
        "Polynomial Regression:\n",
        "The relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial, which allows for curved relationships.\n",
        "The equation for polynomial regression with one predictor\n",
        "ùëã\n",
        "X of degree 2 (quadratic) is:\n",
        "ùëå\n",
        "=\n",
        "ùõΩ\n",
        "0\n",
        "+\n",
        "ùõΩ\n",
        "1\n",
        "ùëã\n",
        "+\n",
        "ùõΩ\n",
        "2\n",
        "ùëã\n",
        "2\n",
        "+\n",
        "ùúñ\n",
        "Y=Œ≤\n",
        "0\n",
        "‚Äã\n",
        " +Œ≤\n",
        "1\n",
        "‚Äã\n",
        " X+Œ≤\n",
        "2\n",
        "‚Äã\n",
        " X\n",
        "2\n",
        " +œµ\n",
        "For higher-degree polynomials, the equation includes higher powers of\n",
        "ùëã\n",
        "X, such as\n",
        "ùëã\n",
        "3\n",
        "X\n",
        "3\n",
        " ,\n",
        "ùëã\n",
        "4\n",
        "X\n",
        "4\n",
        " , etc.\n",
        "Polynomial regression allows for nonlinear relationships between the independent and dependent variables.\n",
        "2. Type of Relationship:\n",
        "Linear Regression: Assumes a linear relationship between the predictors and the response variable. This means that for each unit increase in the independent variable, the dependent variable changes by a constant amount (the slope).\n",
        "\n",
        "Polynomial Regression: Can model nonlinear relationships by introducing higher-degree terms (e.g.,\n",
        "ùëã\n",
        "2\n",
        ",\n",
        "ùëã\n",
        "3\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ), allowing the curve to bend and change direction. It‚Äôs useful when the relationship between the variables is not a straight line.\n",
        "\n"
      ],
      "metadata": {
        "id": "CGZbpVbJNKDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?"
      ],
      "metadata": {
        "id": "KVJL1PvEOkLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is used when the relationship between the independent and dependent variables is nonlinear and cannot be adequately captured by a simple straight line (as in linear regression). Polynomial regression allowed to model more complex curves in the data, making it useful in a variety of situations."
      ],
      "metadata": {
        "id": "HvSoEE_QOuXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "\n"
      ],
      "metadata": {
        "id": "x7mfE0pLOxD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y=Œ≤\n",
        "0\n",
        "‚Äã\n",
        " +Œ≤\n",
        "1\n",
        "‚Äã\n",
        " X+Œ≤\n",
        "2\n",
        "‚Äã\n",
        " X\n",
        "2\n",
        " +Œ≤\n",
        "3\n",
        "‚Äã\n",
        " X\n",
        "3\n",
        " +‚ãØ+Œ≤\n",
        "n\n",
        "‚Äã\n",
        " X\n",
        "n\n",
        " +œµ\n",
        "Where:\n",
        "\n",
        "ùëå\n",
        "Y is the dependent variable (response).\n",
        "ùëã\n",
        "X is the independent variable (predictor).\n",
        "ùõΩ\n",
        "0\n",
        "Œ≤\n",
        "0\n",
        "‚Äã\n",
        "  is the intercept (constant term).\n",
        "ùõΩ\n",
        "1\n",
        ",\n",
        "ùõΩ\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        ",\n",
        "ùõΩ\n",
        "ùëõ\n",
        "Œ≤\n",
        "1\n",
        "‚Äã\n",
        " ,Œ≤\n",
        "2\n",
        "‚Äã\n",
        " ,‚Ä¶,Œ≤\n",
        "n\n",
        "‚Äã\n",
        "  are the coefficients (weights) of the polynomial terms.\n",
        "ùëõ\n",
        "n is the degree of the polynomial (e.g., 2 for quadratic, 3 for cubic, etc.).\n",
        "ùúñ\n",
        "œµ is the error term (residuals), representing the random variation or noise in the data."
      ],
      "metadata": {
        "id": "tz0cU0zLO7e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?"
      ],
      "metadata": {
        "id": "J1dzXJU6PKGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, polynomial regression can be applied to multiple variables, known as multivariate polynomial regression. This approach have also extended the concept of polynomial regression to handle multiple independent variables by including polynomial terms for each predictor as well as interaction terms between them. For example, if you have two variables\n",
        "ùëã\n",
        "1\n",
        "X\n",
        "1\n",
        "‚Äã\n",
        "  and\n",
        "ùëã\n",
        "2\n",
        "X\n",
        "2\n",
        "‚Äã\n",
        " , a quadratic polynomial regression would include terms like\n",
        "ùëã\n",
        "1\n",
        "2\n",
        "X\n",
        "1\n",
        "2\n",
        "‚Äã\n",
        " ,\n",
        "ùëã\n",
        "2\n",
        "2\n",
        "X\n",
        "2\n",
        "2\n",
        "‚Äã\n",
        " , and\n",
        "ùëã\n",
        "1\n",
        "ùëã\n",
        "2\n",
        "X\n",
        "1\n",
        "‚Äã\n",
        " X\n",
        "2\n",
        "‚Äã\n",
        " . This allows the model to capture nonlinear relationships and interactions between multiple predictors, offering a more flexible approach to modeling complex data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z_ZxT_yaQjym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?"
      ],
      "metadata": {
        "id": "XijGqPrHQwTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a powerful tool for modeling nonlinear relationships, but it has several limitations:\n",
        "\n",
        "Overfitting: As the degree of the polynomial increases, the model becomes more flexible, allowing it to fit the training data very closely. However, this also increases the risk of overfitting, where the model captures noise or outliers rather than the true underlying pattern. This results in poor generalization to new, unseen data.\n",
        "\n",
        "Extrapolation Issues: Polynomial regression models can behave erratically when making predictions outside the range of the data, a phenomenon known as extrapolation. High-degree polynomials can lead to wildly fluctuating predictions that do not reflect the real-world behavior of the data.\n",
        "\n",
        "Multicollinearity: In multivariate polynomial regression, higher-degree terms (e.g.,\n",
        "ùëã\n",
        "1\n",
        "2\n",
        "X\n",
        "1\n",
        "2\n",
        "‚Äã\n",
        " ,\n",
        "ùëã\n",
        "1\n",
        "ùëã\n",
        "2\n",
        "X\n",
        "1\n",
        "‚Äã\n",
        " X\n",
        "2\n",
        "‚Äã\n",
        " ) can become highly correlated with one another, which leading to multicollinearity. This makes estimating the regression coefficients less reliable and increases the standard errors.\n",
        "\n",
        "Model Complexity:\n",
        "As more features and higher-degree terms are added, the model becomes increasingly complex. This makes the model harder to interpret and computationally expensive to fit, especially with large datasets.\n",
        "\n",
        "Interpretability:\n",
        "Higher-degree polynomial models can become difficult to interpret due to the increasing number of terms and their interactions, making it harder to understand the specific relationship between predictors and the response variable."
      ],
      "metadata": {
        "id": "9zL-VHrtSN6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n"
      ],
      "metadata": {
        "id": "Hzr1aMz9SY3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation:\n",
        "This technique involves splitting the dataset into training and validation sets. By evaluating model performance on the validation set, cross-validation helps prevent overfitting and provides an unbiased estimate of model performance across different polynomial degrees.\n",
        "Adjusted R¬≤:\n",
        "Unlike R¬≤, which can artificially increase with higher-degree polynomials, Adjusted R¬≤ adjusts for the number of predictors in the model, helping to identify the degree that balances fit and complexity.\n",
        "AIC/BIC (Akaike Information Criterion/Bayesian Information Criterion):\n",
        "These metrics assess model fit while penalizing for excessive complexity. A lower AIC or BIC indicates a better model, considering both goodness of fit and model complexity.\n",
        "Residual Plots:\n",
        "Evaluating the residuals (errors) of the model can help identify patterns that indicate a poor fit. Ideally, residuals should be randomly distributed. Patterns may suggest that the degree of the polynomial needs to be adjusted."
      ],
      "metadata": {
        "id": "wdmPRNkdSiHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?"
      ],
      "metadata": {
        "id": "0x7TCi3hTGxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Fit: Plotting the data points along with the regression curve helps visually inspect whether the polynomial degree chosen captures the underlying trend in the data. It shows whether the model is too simple (underfitting) or too complex (overfitting).\n",
        "\n",
        "Identifying Nonlinearity: Polynomial regression is used to model nonlinear relationships, and visualizing the curve helps confirm if the polynomial captures the correct shape (e.g., quadratic, cubic).\n",
        "\n",
        "Residual Analysis: Residual plots allow you to check if the errors are randomly distributed. Patterns in the residuals could indicate a need for a different degree or model type.\n",
        "\n",
        "Extrapolation: Visualization can highlight problems with extrapolation, especially with high-degree polynomials, where the curve may make unrealistic predictions outside the observed data range."
      ],
      "metadata": {
        "id": "opLHWESiTP8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?"
      ],
      "metadata": {
        "id": "81jM0AGxTXpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Example dataset\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1, 4, 9, 16, 25])\n",
        "\n",
        "# Create polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Visualize the result\n",
        "plt.scatter(X, y, color='red')\n",
        "plt.plot(X, model.predict(X_poly), color='blue')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "jmWNc11_T3mX",
        "outputId": "82152097-0807-4bd4-e4cd-b36530f917a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}